{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Heisnotanimposter/ObjectDetection_with_Server/blob/main/PersonSpeedvision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgJklsACD0f3",
        "outputId": "c6b8c3e2-4b2b-43ee-bafc-736feebf6214"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Aug 18 13:10:55 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3B2zj9HkxNgz",
        "outputId": "586d32be-f850-44ce-d5ac-b29214225a65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCZ_qDnLD87m",
        "outputId": "f281c262-b3a4-401e-c2c2-7af1cc067fb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.0/869.0 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting roboflow\n",
            "  Downloading roboflow-1.1.40-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from roboflow) (2024.7.4)\n",
            "Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.4)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.2)\n",
            "Collecting requests-toolbelt (from roboflow)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.2.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.53.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.2)\n",
            "Downloading roboflow-1.1.40-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, python-dotenv, requests-toolbelt, roboflow\n",
            "Successfully installed filetype-1.2.0 python-dotenv-1.0.1 requests-toolbelt-1.0.0 roboflow-1.1.40\n"
          ]
        }
      ],
      "source": [
        "!pip install -q supervision ultralytics\n",
        "!pip install roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "WWiGuYHh5e8O",
        "outputId": "b5ebb113-ffcf-4945-b0a1-9255bb2e723b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8x.pt to 'yolov8x.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131M/131M [00:03<00:00, 41.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 384x640 3 persons, 1 car, 75.6ms\n",
            "Speed: 14.4ms preprocess, 75.6ms inference, 704.7ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'tuple' object has no attribute 'id'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-beed9f378362>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Annotate frame manually\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrack\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtracks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mtrack_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox\u001b[0m  \u001b[0;31m# Get bounding box coordinates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'id'"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "import supervision as sv\n",
        "from collections import defaultdict, deque\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Load the YOLO model (You can switch to a different YOLO variant if needed)\n",
        "model = YOLO(\"yolov8x.pt\")  # You can use yolov8n.pt, yolov8m.pt, etc.\n",
        "\n",
        "# Set up video paths\n",
        "SOURCE_VIDEO_PATH = \"/content/drive/MyDrive/Team7dataset/Team7Shared/140sNightShinjuku.mp4\"\n",
        "TARGET_VIDEO_PATH = \"/content/drive/MyDrive/Team7dataset/Team7Shared/140sNightShinjuku_yolov8_result.mp4\"\n",
        "\n",
        "# Initialize video capture\n",
        "cap = cv2.VideoCapture(SOURCE_VIDEO_PATH)\n",
        "\n",
        "# Video properties\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(TARGET_VIDEO_PATH, fourcc, fps, (width, height))\n",
        "\n",
        "# Initialize ByteTrack tracker with updated parameters\n",
        "byte_track = sv.ByteTrack(\n",
        "    track_activation_threshold=0.3,\n",
        "    lost_track_buffer=30,\n",
        "    frame_rate=fps\n",
        ")\n",
        "\n",
        "# Initialize data structures to store past positions for speed estimation\n",
        "past_positions = defaultdict(lambda: deque(maxlen=5))\n",
        "\n",
        "# Process the video\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run YOLO object detection\n",
        "    results = model(frame, conf=0.5)  # Adjust confidence threshold as needed\n",
        "\n",
        "    # Get detections\n",
        "    detections = sv.Detections.from_ultralytics(results[0])\n",
        "\n",
        "    # Filter detections by confidence\n",
        "    detections = detections[detections.confidence > 0.3]\n",
        "\n",
        "    # Update tracker with detections\n",
        "    tracks = byte_track.update_with_detections(detections)\n",
        "\n",
        "    # Annotate frame manually\n",
        "    for track in tracks:\n",
        "        track_id = track.id\n",
        "        bbox = track.bbox  # Get bounding box coordinates\n",
        "\n",
        "        if bbox is not None:\n",
        "            # Draw the bounding box\n",
        "            x1, y1, x2, y2 = map(int, bbox)\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "            # Calculate speed estimation\n",
        "            center_x = (x1 + x2) // 2\n",
        "            center_y = (y1 + y2) // 2\n",
        "\n",
        "            if track_id in past_positions:\n",
        "                past_positions[track_id].append((center_x, center_y))\n",
        "                if len(past_positions[track_id]) > 1:\n",
        "                    # Calculate the displacement between the first and last positions\n",
        "                    x_start, y_start = past_positions[track_id][0]\n",
        "                    x_end, y_end = past_positions[track_id][-1]\n",
        "                    distance = np.sqrt((x_end - x_start)**2 + (y_end - y_start)**2)\n",
        "\n",
        "                    # Categorize the speed\n",
        "                    if distance > 50:  # Adjust these thresholds based on your data\n",
        "                        speed_category = \"High Speed\"\n",
        "                        color = (0, 0, 255)  # Red for high speed\n",
        "                    elif 20 < distance <= 50:\n",
        "                        speed_category = \"Mid Speed\"\n",
        "                        color = (0, 255, 255)  # Yellow for mid speed\n",
        "                    else:\n",
        "                        speed_category = \"Low Speed\"\n",
        "                        color = (0, 255, 0)  # Green for low speed\n",
        "\n",
        "                    # Draw the speed category\n",
        "                    cv2.putText(frame, f'{speed_category}', (x1, y1 - 10),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "            else:\n",
        "                past_positions[track_id].append((center_x, center_y))\n",
        "\n",
        "            # Draw the track ID\n",
        "            cv2.putText(frame, f'ID: {track_id}', (x1, y1 - 30),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "\n",
        "    # Write the annotated frame to the output video\n",
        "    out.write(frame)\n",
        "\n",
        "    # Display the frame (optional)\n",
        "    cv2_imshow(frame)  # Use cv2_imshow instead of cv2.imshow\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# Save the results\n",
        "print(f\"Results saved to {TARGET_VIDEO_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6W-jwpQj_OOj"
      },
      "outputs": [],
      "source": [
        "from roboflow import Roboflow\n",
        "import shutil\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "from tqdm import tqdm\n",
        "from ultralytics import YOLO\n",
        "from collections import defaultdict, deque\n",
        "from scipy.spatial import distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sp_KZxF8DaRU"
      },
      "outputs": [],
      "source": [
        "# Crosswalk, 2-wheel dataset\n",
        "!curl -L \"https://universe.roboflow.com/ds/p7t3Nx8tQM?key=Lq5jn3mTlf\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n",
        "\n",
        "# Car dataset\n",
        "!curl -L \"https://universe.roboflow.com/ds/CGpCt0Eh41?key=SK4lZaaD6B\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n",
        "\n",
        "# Person dataset\n",
        "!curl -L \"https://universe.roboflow.com/ds/JrT0ne1aM8?key=pejxGZTe7U\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hP3CtplkDrds"
      },
      "outputs": [],
      "source": [
        "# Paths to the downloaded datasets\n",
        "crosswalk_dataset = \"/content/your_crosswalk_2wheel_dataset_path\"\n",
        "car_dataset = \"/content/your_car_dataset_path\"\n",
        "person_dataset = \"/content/your_person_dataset_path\"\n",
        "\n",
        "# Unified dataset path\n",
        "unified_dataset_path = \"/content/dataset\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(f\"{unified_dataset_path}/train/images\", exist_ok=True)\n",
        "os.makedirs(f\"{unified_dataset_path}/train/labels\", exist_ok=True)\n",
        "os.makedirs(f\"{unified_dataset_path}/valid/images\", exist_ok=True)\n",
        "os.makedirs(f\"{unified_dataset_path}/valid/labels\", exist_ok=True)\n",
        "os.makedirs(f\"{unified_dataset_path}/test/images\", exist_ok=True)\n",
        "os.makedirs(f\"{unified_dataset_path}/test/labels\", exist_ok=True)\n",
        "\n",
        "# Function to copy files\n",
        "def copy_files(src, dst, subdir):\n",
        "    for split in [\"train\", \"valid\", \"test\"]:\n",
        "        images_src = f\"{src}/{split}/images\"\n",
        "        labels_src = f\"{src}/{split}/labels\"\n",
        "\n",
        "        images_dst = f\"{dst}/{split}/images\"\n",
        "        labels_dst = f\"{dst}/{split}/labels\"\n",
        "\n",
        "        if os.path.exists(images_src):\n",
        "            for file_name in os.listdir(images_src):\n",
        "                shutil.copy(f\"{images_src}/{file_name}\", f\"{images_dst}/{file_name}\")\n",
        "        if os.path.exists(labels_src):\n",
        "            for file_name in os.listdir(labels_src):\n",
        "                shutil.copy(f\"{labels_src}/{file_name}\", f\"{labels_dst}/{file_name}\")\n",
        "\n",
        "# Copy files from each dataset to the unified dataset\n",
        "copy_files(crosswalk_dataset, unified_dataset_path, \"crosswalk\")\n",
        "copy_files(car_dataset, unified_dataset_path, \"car\")\n",
        "copy_files(person_dataset, unified_dataset_path, \"person\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGodr23knGGw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uoc-0fvPy6HH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Check that the train, val, and test directories exist and list their contents\n",
        "train_dir = \"/content/People-Detection-8/train/images\"\n",
        "val_dir = \"/content/People-Detection-8/valid/images\"\n",
        "test_dir = \"/content/People-Detection-8/test/images\"\n",
        "\n",
        "print(\"Train Directory Exists:\", os.path.exists(train_dir))\n",
        "print(\"Validation Directory Exists:\", os.path.exists(val_dir))\n",
        "print(\"Test Directory Exists:\", os.path.exists(test_dir))\n",
        "\n",
        "if os.path.exists(train_dir):\n",
        "    print(\"Train Directory Content:\", os.listdir(train_dir)[:5])  # Show first 5 files\n",
        "if os.path.exists(val_dir):\n",
        "    print(\"Validation Directory Content:\", os.listdir(val_dir)[:5])\n",
        "if os.path.exists(test_dir):\n",
        "    print(\"Test Directory Content:\", os.listdir(test_dir)[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioNpnazIw_0H"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"yolov8n.pt\")  # Start with a pre-trained YOLOv8 model\n",
        "\n",
        "# Train the model with your dataset\n",
        "# Need to modify the data.yaml with specific target location\n",
        "\"\"\"\n",
        "data.yaml example:\n",
        "names:\n",
        "- person\n",
        "nc: 1\n",
        "roboflow:\n",
        "  license: Private\n",
        "  project: people-detection-o4rdr\n",
        "  url: https://universe.roboflow.com/leo-ueno/people-detection-o4rdr/dataset/8\n",
        "  version: 8\n",
        "  workspace: leo-ueno\n",
        "test: /content/People-Detection-8/test/images\n",
        "train: /content/People-Detection-8/train/images\n",
        "val: /content/People-Detection-8/valid/images\n",
        "\"\"\"\n",
        "\n",
        "model.train(data=\"/content/drive/MyDrive/Team7dataset/Team7Shared/data.yaml\", epochs=3, imgsz=480, name=\"people_detection_model\")\n",
        "\n",
        "#/content/drive/MyDrive/Team7dataset/Team7Shared/data.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l47UBH6YxEpj"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define the source and destination paths\n",
        "best_model_path = \"/content/runs/detect/people_detection_model3/weights/best.pt\"\n",
        "destination_path = \"/content/drive/MyDrive/Team7dataset/best.pt\"\n",
        "\n",
        "# Check if the source path exists\n",
        "if os.path.exists(best_model_path):\n",
        "    # Copy the best model to the desired location\n",
        "    shutil.copy(best_model_path, destination_path)\n",
        "    print(f\"Model successfully copied to {destination_path}\")\n",
        "else:\n",
        "    print(f\"Best model not found at {best_model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv_COh1SHvIY"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT2qm1AIHxMf"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "\n",
        "from tqdm import tqdm\n",
        "from ultralytics import YOLO\n",
        "#from supervision.assets import VideoAssets, download_assets\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gb-C2YmHObf"
      },
      "outputs": [],
      "source": [
        "#download_assets(VideoAssets.VEHICLES)\n",
        "#!pip install -q gdown\n",
        "#%cd {HOME}\n",
        "#!gdown '1pz68D1Gsx80MoPg-_q-IbEdESEmyVLm-'\n",
        "#SOURCE_VIDEO_PATH = f\"{HOME}/Day2024_Tokyo_Shinjuku_20240810_162047.mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYfCWNrgIKLo"
      },
      "outputs": [],
      "source": [
        "SOURCE_VIDEO_PATH = \"/content/drive/MyDrive/Team7dataset/DayShinjuku.mp4\"\n",
        "TARGET_VIDEO_PATH = \"/content/drive/MyDrive/Team7dataset/DayShinjuku.mp4_result.mp4\"\n",
        "CONFIDENCE_THRESHOLD = 0.3\n",
        "IOU_THRESHOLD = 0.5\n",
        "MODEL_NAME = \"yolov8S.pt\"\n",
        "MODEL_RESOLUTION = 480"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23BVdBd2HRpn"
      },
      "outputs": [],
      "source": [
        "class ViewTransformer:\n",
        "    def __init__(self, source: np.ndarray, target: np.ndarray) -> None:\n",
        "        source = source.astype(np.float32)\n",
        "        target = target.astype(np.float32)\n",
        "        self.m = cv2.getPerspectiveTransform(source, target)\n",
        "\n",
        "    def transform_points(self, points: np.ndarray) -> np.ndarray:\n",
        "        if points.size == 0:\n",
        "            return points\n",
        "\n",
        "        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)\n",
        "        transformed_points = cv2.perspectiveTransform(reshaped_points, self.m)\n",
        "        return transformed_points.reshape(-1, 2)\n",
        "\n",
        "# Define multiple source areas for crosswalks\n",
        "source_areas = [\n",
        "    np.array([[600, 300], [1200, 300], [1200, 600], [600, 600]]),  # Crosswalk 1\n",
        "    np.array([[0, 450], [100, 450], [100, 550], [0, 550]]), # Crosswalk 2\n",
        "    np.array([[150, 150], [450, 150], [450, 250], [150, 250]]),   # Crosswalk 3\n",
        "    np.array([[600, 100], [700, 100], [700, 200], [600, 200]]),   # Crosswalk 4\n",
        "    #np.array([[150, 250], [450, 250], [450, 350], [150, 350]]),   # Crosswalk 5\n",
        "]\n",
        "\n",
        "# Define a single target area (rectangle) for the perspective transformation\n",
        "TARGET_WIDTH = 300  # Adjust to your needs\n",
        "TARGET_HEIGHT = 200  # Adjust to your needs\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(source_path=SOURCE_VIDEO_PATH)\n",
        "frame_iterator = iter(frame_generator)\n",
        "frame = next(frame_iterator)\n",
        "\n",
        "target_area = np.array([\n",
        "    [0, 0],\n",
        "    [TARGET_WIDTH - 1, 0],\n",
        "    [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
        "    [0, TARGET_HEIGHT - 1],\n",
        "], dtype=np.float32)\n",
        "\n",
        "# Assuming you have a 'frame' loaded, for example, from a video or image\n",
        "#frame = cv2.imread(SOURCE_VIDEO_PATH)\n",
        "\n",
        "# Process each crosswalk\n",
        "for i, source in enumerate(source_areas):\n",
        "    view_transformer = ViewTransformer(source=source, target=target_area)\n",
        "\n",
        "    # Apply perspective transformation to the entire frame for the current crosswalk\n",
        "    transformed_frame = cv2.warpPerspective(frame, view_transformer.m, (TARGET_WIDTH, TARGET_HEIGHT))\n",
        "\n",
        "    # Display the transformed frame (only in Colab)\n",
        "    from google.colab.patches import cv2_imshow\n",
        "    print(f\"Transformed Crosswalk {i+1}:\")\n",
        "    cv2_imshow(transformed_frame)\n",
        "\n",
        "    # Example operation: print the transformed points\n",
        "    print(f\"Crosswalk {i+1} transformed points:\")\n",
        "    print(view_transformer.transform_points(source))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJFj0duTXE3H"
      },
      "outputs": [],
      "source": [
        "#frame_generator = sv.get_video_frames_generator(source_path=SOURCE_VIDEO_PATH)\n",
        "#frame_iterator = iter(frame_generator)\n",
        "#frame = next(frame_iterator)\n",
        "# Annotate the original frame with the polygon for the current source area\n",
        "annotated_frame = frame.copy()\n",
        "color = (0, 0, 255)  # RGB\n",
        "for src in source_areas:\n",
        "    cv2.polylines(annotated_frame, [src.astype(np.int32)], isClosed=True, color=color, thickness=4)\n",
        "\n",
        "# Display the annotated frame\n",
        "cv2_imshow(annotated_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLKeci5YLcx1"
      },
      "source": [
        "## Transform Perspective"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dU--OuJKkZT"
      },
      "outputs": [],
      "source": [
        "class ViewTransformer:\n",
        "\n",
        "    def __init__(self, source: np.ndarray, target: np.ndarray) -> None:\n",
        "        source = source.astype(np.float32)\n",
        "        target = target.astype(np.float32)\n",
        "        self.m = cv2.getPerspectiveTransform(source, target)\n",
        "\n",
        "    def transform_points(self, points: np.ndarray) -> np.ndarray:\n",
        "        if points.size == 0:\n",
        "            return points\n",
        "\n",
        "        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)\n",
        "        transformed_points = cv2.perspectiveTransform(reshaped_points, self.m)\n",
        "        return transformed_points.reshape(-1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtKhXCeFQfc4"
      },
      "outputs": [],
      "source": [
        "#view_transformer = ViewTransformer(source=SOURCE, target=TARGET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIyG3oz5RhWO"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "model = YOLO(\"yolov8s.pt\") # load the model\n",
        "results = model.train(data=\"/content/People-Detection-8/data.yaml\", epochs=5)\n",
        "results = model(\"/content/drive/MyDrive/Team7dataset/140sDayShinjuku.mp4\")\n",
        "\n",
        "\n",
        "model = YOLO(MODEL_NAME)\n",
        "\n",
        "video_info = sv.VideoInfo.from_video_path(video_path=SOURCE_VIDEO_PATH)\n",
        "frame_generator = sv.get_video_frames_generator(source_path=SOURCE_VIDEO_PATH)\n",
        "\n",
        "# tracer initiation\n",
        "byte_track = sv.ByteTrack(\n",
        "    frame_rate=video_info.fps, track_thresh=CONFIDENCE_THRESHOLD\n",
        ")\n",
        "\n",
        "# annotators configuration\n",
        "thickness = sv.calculate_dynamic_line_thickness(\n",
        "    resolution_wh=video_info.resolution_wh\n",
        ")\n",
        "text_scale = sv.calculate_dynamic_text_scale(\n",
        "    resolution_wh=video_info.resolution_wh\n",
        ")\n",
        "bounding_box_annotator = sv.BoundingBoxAnnotator(\n",
        "    thickness=thickness\n",
        ")\n",
        "label_annotator = sv.LabelAnnotator(\n",
        "    text_scale=text_scale,\n",
        "    text_thickness=thickness,\n",
        "    text_position=sv.Position.BOTTOM_CENTER\n",
        ")\n",
        "trace_annotator = sv.TraceAnnotator(\n",
        "    thickness=thickness,\n",
        "    trace_length=video_info.fps * 2,\n",
        "    position=sv.Position.BOTTOM_CENTER\n",
        ")\n",
        "\n",
        "polygon_zone = sv.PolygonZone(\n",
        "    polygon=SOURCE,\n",
        "    frame_resolution_wh=video_info.resolution_wh\n",
        ")\n",
        "\n",
        "coordinates = defaultdict(lambda: deque(maxlen=video_info.fps))\n",
        "\n",
        "# open target video\n",
        "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
        "\n",
        "    # loop over source video frame\n",
        "    for frame in tqdm(frame_generator, total=video_info.total_frames):\n",
        "\n",
        "        result = model(frame, imgsz=MODEL_RESOLUTION, verbose=False)[0]\n",
        "        detections = sv.Detections.from_ultralytics(result)\n",
        "\n",
        "        # filter out detections by class and confidence\n",
        "        detections = detections[detections.confidence > CONFIDENCE_THRESHOLD]\n",
        "        detections = detections[detections.class_id != 0]\n",
        "\n",
        "        # filter out detections outside the zone\n",
        "        detections = detections[polygon_zone.trigger(detections)]\n",
        "\n",
        "        # refine detections using non-max suppression\n",
        "        detections = detections.with_nms(IOU_THRESHOLD)\n",
        "\n",
        "        # pass detection through the tracker\n",
        "        detections = byte_track.update_with_detections(detections=detections)\n",
        "\n",
        "        points = detections.get_anchors_coordinates(\n",
        "            anchor=sv.Position.BOTTOM_CENTER\n",
        "        )\n",
        "\n",
        "        # calculate the detections position inside the target RoI\n",
        "        points = view_transformer.transform_points(points=points).astype(int)\n",
        "\n",
        "        # store detections position\n",
        "        for tracker_id, [_, y] in zip(detections.tracker_id, points):\n",
        "            coordinates[tracker_id].append(y)\n",
        "\n",
        "        # format labels\n",
        "        labels = []\n",
        "\n",
        "        for tracker_id in detections.tracker_id:\n",
        "            if len(coordinates[tracker_id]) < video_info.fps / 2:\n",
        "                labels.append(f\"#{tracker_id}\")\n",
        "            else:\n",
        "                # calculate speed\n",
        "                coordinate_start = coordinates[tracker_id][-1]\n",
        "                coordinate_end = coordinates[tracker_id][0]\n",
        "                distance = abs(coordinate_start - coordinate_end)\n",
        "                time = len(coordinates[tracker_id]) / video_info.fps\n",
        "                speed = distance / time * 3.6\n",
        "                labels.append(f\"#{tracker_id} {int(speed)} km/h\")\n",
        "\n",
        "        # annotate frame\n",
        "        annotated_frame = frame.copy()\n",
        "        annotated_frame = trace_annotator.annotate(\n",
        "            scene=annotated_frame, detections=detections\n",
        "        )\n",
        "        annotated_frame = bounding_box_annotator.annotate(\n",
        "            scene=annotated_frame, detections=detections\n",
        "        )\n",
        "        annotated_frame = label_annotator.annotate(\n",
        "            scene=annotated_frame, detections=detections, labels=labels\n",
        "        )\n",
        "\n",
        "        # add frame to target video\n",
        "        sink.write_frame(annotated_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Su-ZEDw0kBj2"
      },
      "outputs": [],
      "source": [
        "# prompt: tensorboard\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs/detect/train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URvAZCt1dz0A"
      },
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "model = YOLO(\"/content/drive/MyDrive/Team7dataset/Team7Shared/person_best2.pt\")\n",
        "\n",
        "# Run validation on the model (using the validation data)\n",
        "results = model.val()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mZJ0kkemBnM"
      },
      "outputs": [],
      "source": [
        "#model.train(data=\"/content/drive/MyDrive/Team7dataset/Team7Shared/data.yaml\", epochs=32, imgsz=480, name=\"people_detection_model\")\n",
        "\n",
        "# Test the model on a new image\n",
        "results = model.predict(\"/content/drive/MyDrive/Team7dataset/Team7Shared/140sDayShinjuku.mp4\", imgsz=320, conf=0.5, iou=0.5, batch=16)\n",
        "\n",
        "# Display results\n",
        "results.show()\n",
        "\n",
        "# Save results\n",
        "results.save(\"/content/drive/MyDrive/Team7dataset/DayShinjuku_result.mp4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX28B3NhmRjN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}